{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7e675d",
   "metadata": {},
   "source": [
    "# Optiver Realized Volatility Prediction - Train\n",
    "\n",
    "**This notebook seeks to EDITS HERE**\n",
    "---------\n",
    "\n",
    "## Files\n",
    "**book_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n",
    "\n",
    " - `stock_id` - ID code for the stock. Not all `stock_id`s exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n",
    " - `time_id` - ID code for the time bucket. `time_id`s are not necessarily sequential but are consistent across all stocks.\n",
    " - `seconds_in_bucket` - Number of seconds from the start of the bucket, always starting from 0.\n",
    " - `bid_price[1/2]` - Normalized prices of the most/second most competitive buy level.\n",
    " - `ask_price[1/2]` - Normalized prices of the most/second most competitive sell level.\n",
    " - `bid_size[1/2]` - The number of shares on the most/second most competitive buy level.\n",
    " - `ask_size[1/2]` - The number of shares on the most/second most competitive sell level.\n",
    " \n",
    "**trade_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `seconds_in_bucket` - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n",
    " - `price` - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n",
    " - `size` - The sum number of shares traded.\n",
    " - `order_count` - The number of unique trade orders taking place.\n",
    " \n",
    "**train.csv** The ground truth values for the training set.\n",
    "\n",
    " - `stock_id` - Same as above, but since this is a csv the column will load as an integer instead of categorical.\n",
    " - `time_id` - Same as above.\n",
    " - `target` - The realized volatility computed over the 10 minute window following the feature data under the same `stock_id`/`time_id`. There is no overlap between feature and target data. \n",
    " \n",
    "**test.csv** Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `row_id` - Unique identifier for the submission row. There is one row for each existing `stock_id`/`time_id` pair. Each time window is not necessarily containing every individual stock.\n",
    " \n",
    "**sample_submission.csv** - A sample submission file in the correct format.\n",
    "\n",
    " - `row_id` - Same as in test.csv.\n",
    " - `target` - Same definition as in **train.csv**. The benchmark is using the median target value from **train.csv**.\n",
    " \n",
    "## Prepare Environment\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dcab26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq # To handle parquet files\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data prep\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# Model cross validation and evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de666dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Num CPU Threads Available: 64\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "\n",
    "# Get number of cpu cores for multiprocessing\n",
    "try:\n",
    "    cpus = int(multiprocessing.cpu_count() / 2)\n",
    "except NotImplementedError:\n",
    "    cpus = 1 # Default number of cores\n",
    "    \n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(f\"Num CPU Threads Available: {cpus}\")\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e027214",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ebc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "comp_dir_path = Path(\"../input/optiver-realized-volatility-prediction\")\n",
    "\n",
    "# Train paths\n",
    "train_book_path   = comp_dir_path/\"book_train.parquet\"\n",
    "train_trade_path  = comp_dir_path/\"trade_train.parquet\"\n",
    "train_labels_path = comp_dir_path/\"train.csv\"\n",
    "\n",
    "# Test paths\n",
    "test_book_path   = comp_dir_path/\"book_test.parquet\"\n",
    "test_trade_path  = comp_dir_path/\"trade_test.parquet\"\n",
    "test_labels_path = comp_dir_path/\"test.csv\"\n",
    "\n",
    "# Sample submission path\n",
    "sample_sub_path = comp_dir_path/\"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d99a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_ids_list(data_dir_path):\n",
    "    data_dir = os.listdir(data_dir_path)\n",
    "    # Get list of stock ids in directory\n",
    "    stock_ids = list(map(lambda x: x.split(\"=\")[1], data_dir))\n",
    "    return stock_ids\n",
    "    \n",
    "    \n",
    "def load_book_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        book_stock_id_path = os.path.join(train_book_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        book_stock_id_path = os.path.join(test_book_path, stock_id_ext)\n",
    "    book_stock_id = pd.read_parquet(book_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    book_stock_id[\"stock_id\"] = stock_id\n",
    "            \n",
    "    return book_stock_id\n",
    "\n",
    "def load_trade_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        trade_stock_id_path = os.path.join(train_trade_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        trade_stock_id_path = os.path.join(test_trade_path, stock_id_ext)\n",
    "    trade_stock_id = pd.read_parquet(trade_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    trade_stock_id[\"stock_id\"] = stock_id\n",
    "            \n",
    "    return trade_stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "394aedc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA DIMENSIONS\n",
      "train_book shape: (2425085, 11)\n",
      "train_trade shape: (419653, 6)\n",
      "train_labels shape: (428932, 3)\n",
      "\n",
      "TEST DATA DIMENSIONS\n",
      "test_book shape: (3, 11)\n",
      "test_trade shape: (3, 6)\n",
      "test_labels shape: (3, 3)\n",
      "\n",
      "CPU times: user 241 ms, sys: 1.97 s, total: 2.21 s\n",
      "Wall time: 2.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get list of stock ids\n",
    "train_stock_ids = get_stock_ids_list(train_book_path)\n",
    "test_stock_ids = get_stock_ids_list(test_book_path)\n",
    "\n",
    "# Read train data\n",
    "is_train_test = \"train\"\n",
    "# Create worker pool and read\n",
    "pool         = multiprocessing.Pool(processes=cpus)\n",
    "train_book   = pd.concat(pool.map(load_book_stock_id_data, train_stock_ids[0:2]))\n",
    "train_trade  = pd.concat(pool.map(load_trade_stock_id_data, train_stock_ids[0:2]))\n",
    "train_labels = pd.read_csv(train_labels_path)\n",
    "# Close worker pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Read test data\n",
    "is_train_test = \"test\"\n",
    "# Create worker pool and read\n",
    "pool        = multiprocessing.Pool(processes=cpus)\n",
    "test_book   = pd.concat(pool.map(load_book_stock_id_data, test_stock_ids))\n",
    "test_trade  = pd.concat(pool.map(load_trade_stock_id_data, test_stock_ids))\n",
    "test_labels = pd.read_csv(test_labels_path)\n",
    "\n",
    "# Read sample submission\n",
    "sample_sub = pd.read_csv(sample_sub_path)\n",
    "\n",
    "# Print data dimensions\n",
    "print(\"TRAIN DATA DIMENSIONS\")\n",
    "print(f\"train_book shape: {train_book.shape}\")\n",
    "print(f\"train_trade shape: {train_trade.shape}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}\")\n",
    "\n",
    "print(\"\\nTEST DATA DIMENSIONS\")\n",
    "print(f\"test_book shape: {test_book.shape}\")\n",
    "print(f\"test_trade shape: {test_trade.shape}\")\n",
    "print(f\"test_labels shape: {test_labels.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7b7fa",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83f51566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for data manipulation\n",
    "def get_log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def get_wap(df_book, col_bid_price, col_ask_price):\n",
    "    \"\"\"\n",
    "    Returns Weighted Average Price. \n",
    "    \"\"\"\n",
    "    wap_numerator   = df_book[col_bid_price] * df_book[col_ask_price] + \n",
    "                        df_book[col_ask_price] * df_book[col_bid_price]\n",
    "    wap_denominator = df_book[col_bid_price] + df_book[col_ask_price]\n",
    "    return wap_numerator / wap_denominator\n",
    "\n",
    "\n",
    "def get_vol_wap(df_book, col_stock_id, col_time_id, col_wap):\n",
    "    \"\"\"\n",
    "    Returns the Volume Weighted Average Price at each time ID.\n",
    "    \"\"\"\n",
    "    vol_wap = df_book.groupby(by=[col_stock_id, col_time_id])[col_wap].apply(get_log_return)\n",
    "    vol_wap = vol_wap.fillna(0)\n",
    "    return vol_wap\n",
    "\n",
    "\n",
    "def get_trade_log_return(df_trade, col_stock_id, col_time_id, col_price):\n",
    "    \"\"\"\n",
    "    Returns the Log Return at each time ID\n",
    "    \"\"\"\n",
    "    trade_log_return = df_trade.groupby([col_stock_id, col_time_id])[col_price].apply(get_log_return)\n",
    "    trade_log_return = trade_log_return.fillna(0)\n",
    "    return trade_log_return\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the Root Mean Squared Prediction Error\n",
    "    \"\"\"\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cffbb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>order_count</th>\n",
       "      <th>stock_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>1.002301</td>\n",
       "      <td>326</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>1.002778</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>1.002818</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>1.003155</td>\n",
       "      <td>121</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>1.003646</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296205</th>\n",
       "      <td>32767</td>\n",
       "      <td>579</td>\n",
       "      <td>0.999010</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296206</th>\n",
       "      <td>32767</td>\n",
       "      <td>587</td>\n",
       "      <td>0.999109</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296207</th>\n",
       "      <td>32767</td>\n",
       "      <td>588</td>\n",
       "      <td>0.999010</td>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296208</th>\n",
       "      <td>32767</td>\n",
       "      <td>592</td>\n",
       "      <td>0.999109</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296209</th>\n",
       "      <td>32767</td>\n",
       "      <td>593</td>\n",
       "      <td>0.998911</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>419653 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_id  seconds_in_bucket     price  size  order_count stock_id\n",
       "0             5                 21  1.002301   326           12        0\n",
       "1             5                 46  1.002778   128            4        0\n",
       "2             5                 50  1.002818    55            1        0\n",
       "3             5                 57  1.003155   121            5        0\n",
       "4             5                 68  1.003646     4            1        0\n",
       "...         ...                ...       ...   ...          ...      ...\n",
       "296205    32767                579  0.999010    81            3        1\n",
       "296206    32767                587  0.999109    50            1        1\n",
       "296207    32767                588  0.999010   126            2        1\n",
       "296208    32767                592  0.999109     1            1        1\n",
       "296209    32767                593  0.998911     1            1        1\n",
       "\n",
       "[419653 rows x 6 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8a099d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0a0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c1e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f21f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f41548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0872df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21c9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d84e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9060c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67a810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124a501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6912414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6b9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c39fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
