{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7e675d",
   "metadata": {},
   "source": [
    "# Optiver Realized Volatility Prediction - Train\n",
    "\n",
    "**This notebook seeks to EDITS HERE**\n",
    "---------\n",
    "\n",
    "## Files\n",
    "**book_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n",
    "\n",
    " - `stock_id` - ID code for the stock. Not all `stock_id`s exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n",
    " - `time_id` - ID code for the time bucket. `time_id`s are not necessarily sequential but are consistent across all stocks.\n",
    " - `seconds_in_bucket` - Number of seconds from the start of the bucket, always starting from 0.\n",
    " - `bid_price[1/2]` - Normalized prices of the most/second most competitive buy level.\n",
    " - `ask_price[1/2]` - Normalized prices of the most/second most competitive sell level.\n",
    " - `bid_size[1/2]` - The number of shares on the most/second most competitive buy level.\n",
    " - `ask_size[1/2]` - The number of shares on the most/second most competitive sell level.\n",
    " \n",
    "**trade_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `seconds_in_bucket` - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n",
    " - `price` - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n",
    " - `size` - The sum number of shares traded.\n",
    " - `order_count` - The number of unique trade orders taking place.\n",
    " \n",
    "**train.csv** The ground truth values for the training set.\n",
    "\n",
    " - `stock_id` - Same as above, but since this is a csv the column will load as an integer instead of categorical.\n",
    " - `time_id` - Same as above.\n",
    " - `target` - The realized volatility computed over the 10 minute window following the feature data under the same `stock_id`/`time_id`. There is no overlap between feature and target data. \n",
    " \n",
    "**test.csv** Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `row_id` - Unique identifier for the submission row. There is one row for each existing `stock_id`/`time_id` pair. Each time window is not necessarily containing every individual stock.\n",
    " \n",
    "**sample_submission.csv** - A sample submission file in the correct format.\n",
    "\n",
    " - `row_id` - Same as in test.csv.\n",
    " - `target` - Same definition as in **train.csv**. The benchmark is using the median target value from **train.csv**.\n",
    " \n",
    "## Prepare Environment\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcab26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq # To handle parquet files\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data prep\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# Model cross validation and evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de666dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Num CPU Threads Available: 64\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "\n",
    "# Get number of cpu cores for multiprocessing\n",
    "try:\n",
    "    cpus = int(multiprocessing.cpu_count() / 2)\n",
    "except NotImplementedError:\n",
    "    cpus = 1 # Default number of cores\n",
    "    \n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(f\"Num CPU Threads Available: {cpus}\")\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e027214",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ebc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "comp_dir_path = Path(\"../input/optiver-realized-volatility-prediction\")\n",
    "\n",
    "# Train paths\n",
    "train_book_path   = comp_dir_path/\"book_train.parquet\"\n",
    "train_trade_path  = comp_dir_path/\"trade_train.parquet\"\n",
    "train_labels_path = comp_dir_path/\"train.csv\"\n",
    "\n",
    "# Test paths\n",
    "test_book_path   = comp_dir_path/\"book_test.parquet\"\n",
    "test_trade_path  = comp_dir_path/\"trade_test.parquet\"\n",
    "test_labels_path = comp_dir_path/\"test.csv\"\n",
    "\n",
    "# Sample submission path\n",
    "sample_sub_path = comp_dir_path/\"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d99a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for data reading\n",
    "def get_stock_ids_list(data_dir_path):\n",
    "    data_dir = os.listdir(data_dir_path)\n",
    "    # Get list of stock ids in directory\n",
    "    stock_ids = list(map(lambda x: x.split(\"=\")[1], data_dir))\n",
    "    return stock_ids\n",
    "    \n",
    "    \n",
    "def load_book_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        book_stock_id_path = os.path.join(train_book_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        book_stock_id_path = os.path.join(test_book_path, stock_id_ext)\n",
    "    book_stock_id = pd.read_parquet(book_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    book_stock_id[\"stock_id\"] = int(stock_id)\n",
    "            \n",
    "    return book_stock_id\n",
    "\n",
    "def load_trade_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        trade_stock_id_path = os.path.join(train_trade_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        trade_stock_id_path = os.path.join(test_trade_path, stock_id_ext)\n",
    "    trade_stock_id = pd.read_parquet(trade_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    trade_stock_id[\"stock_id\"] = int(stock_id)\n",
    "            \n",
    "    return trade_stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394aedc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA DIMENSIONS\n",
      "train_book shape: (2425085, 11)\n",
      "train_trade shape: (419653, 6)\n",
      "train_labels shape: (428932, 3)\n",
      "\n",
      "TEST DATA DIMENSIONS\n",
      "test_book shape: (3, 11)\n",
      "test_trade shape: (3, 6)\n",
      "test_labels shape: (3, 3)\n",
      "\n",
      "CPU times: user 144 ms, sys: 783 ms, total: 926 ms\n",
      "Wall time: 1.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get list of stock ids\n",
    "train_stock_ids = get_stock_ids_list(train_book_path)\n",
    "test_stock_ids = get_stock_ids_list(test_book_path)\n",
    "\n",
    "# Read train data\n",
    "is_train_test = \"train\"\n",
    "# Create worker pool and read\n",
    "pool         = multiprocessing.Pool(processes=cpus)\n",
    "train_book   = pd.concat(pool.map(load_book_stock_id_data, train_stock_ids[0:2]))\n",
    "train_trade  = pd.concat(pool.map(load_trade_stock_id_data, train_stock_ids[0:2]))\n",
    "train_labels = pd.read_csv(train_labels_path)\n",
    "# Close worker pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Read test data\n",
    "is_train_test = \"test\"\n",
    "# Create worker pool and read\n",
    "pool        = multiprocessing.Pool(processes=cpus)\n",
    "test_book   = pd.concat(pool.map(load_book_stock_id_data, test_stock_ids))\n",
    "test_trade  = pd.concat(pool.map(load_trade_stock_id_data, test_stock_ids))\n",
    "test_labels = pd.read_csv(test_labels_path)\n",
    "\n",
    "# Read sample submission\n",
    "sample_sub = pd.read_csv(sample_sub_path)\n",
    "\n",
    "# Print data dimensions\n",
    "print(\"TRAIN DATA DIMENSIONS\")\n",
    "print(f\"train_book shape: {train_book.shape}\")\n",
    "print(f\"train_trade shape: {train_trade.shape}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}\")\n",
    "\n",
    "print(\"\\nTEST DATA DIMENSIONS\")\n",
    "print(f\"test_book shape: {test_book.shape}\")\n",
    "print(f\"test_trade shape: {test_trade.shape}\")\n",
    "print(f\"test_labels shape: {test_labels.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7b7fa",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Define Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f51566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for data manipulation\n",
    "def get_log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def get_trade_log_return(df_trade, col_stock_id, col_time_id, col_price):\n",
    "    \"\"\"\n",
    "    Returns the Log Return at each time ID.\n",
    "    \"\"\"\n",
    "    trade_log_return = df_trade.groupby([col_stock_id, col_time_id])[col_price].apply(get_log_return)\n",
    "    trade_log_return = trade_log_return.fillna(0)\n",
    "    return trade_log_return\n",
    "\n",
    "\n",
    "def get_agg_feature(df, col_name, func):\n",
    "    \"\"\"\n",
    "    Returns aggregated feature by stock ID and time ID based on input df and feature.\n",
    "    \"\"\"\n",
    "    if \"function\" in str(func):\n",
    "        func_str = str(func).split(\" \")[1]\n",
    "        agg_feat_col_name = f\"{col_name}_{func_str}\"\n",
    "    else:\n",
    "        agg_feat_col_name = f\"{col_name}_{func}\"\n",
    "    \n",
    "    agg_feat = df.groupby(by=[\"stock_id\", \"time_id\"])[col_name].agg(func)\n",
    "    agg_feat = agg_feat.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    agg_feat = agg_feat.reset_index().rename(columns={col_name: agg_feat_col_name})\n",
    "    \n",
    "    return agg_feat\n",
    "\n",
    "\n",
    "def get_wap(df_book, col_bid_price, col_ask_price, col_bid_size, col_ask_size):\n",
    "    \"\"\"\n",
    "    Returns Weighted Average Price. \n",
    "    \"\"\"\n",
    "    wap_numerator = df_book[col_bid_price]  * df_book[col_ask_size]\n",
    "    wap_numerator += df_book[col_ask_price] * df_book[col_bid_size]\n",
    "    \n",
    "    wap_denominator = df_book[col_bid_size] + df_book[col_ask_size]\n",
    "    \n",
    "    return wap_numerator / wap_denominator\n",
    "\n",
    "\n",
    "def get_wap_combined(df_book, col_bid_price1, col_ask_price1, col_bid_size1, col_ask_size1,\n",
    "                     col_bid_price2, col_ask_price2, col_bid_size2, col_ask_size2):    \n",
    "    \"\"\"\n",
    "    Returns the Combined Weighted Average Price for both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    wap_numerator1  = df_book[col_bid_price1] * df_book[col_ask_size1]\n",
    "    wap_numerator1 += df_book[col_ask_price1] * df_book[col_bid_size1]\n",
    "    wap_numerator2  = df_book[col_bid_price2] * df_book[col_ask_size2]\n",
    "    wap_numerator2 += df_book[col_ask_price2] * df_book[col_bid_size2]\n",
    "    \n",
    "    wap_denominator  = df_book[col_bid_size1] + df_book[col_ask_size1]\n",
    "    wap_denominator += df_book[col_bid_size2] + df_book[col_ask_size2]\n",
    "    \n",
    "    return (wap_numerator1 + wap_numerator2) / wap_denominator\n",
    "\n",
    "\n",
    "def get_wap_avg(df_book, col_bid_price1, col_ask_price1, col_bid_size1, col_ask_size1,\n",
    "                col_bid_price2, col_ask_price2, col_bid_size2, col_ask_size2):\n",
    "    \"\"\"\n",
    "    Returns the Combined Average Weighted Average Price for both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    wap_numerator1  = df_book[col_bid_price1] * df_book[col_ask_size1]\n",
    "    wap_numerator1 += df_book[col_ask_price1] * df_book[col_bid_size1]\n",
    "    wap_numerator1 /= df_book[col_bid_size1] + df_book[col_ask_size1]\n",
    "    \n",
    "    wap_numerator2  = df_book[col_bid_price2] * df_book[col_ask_size2]\n",
    "    wap_numerator2 += df_book[col_ask_price2] * df_book[col_bid_size2]\n",
    "    wap_numerator2 /= df_book[col_bid_size2] + df_book[col_ask_size2]\n",
    "    \n",
    "    return (wap_numerator1 + wap_numerator2) / 2\n",
    "\n",
    "\n",
    "def get_vol_wap(df_book, col_stock_id, col_time_id, col_wap):\n",
    "    \"\"\"\n",
    "    Returns the Volume Weighted Average Price at each time ID.\n",
    "    \"\"\"\n",
    "    vol_wap = df_book.groupby([col_stock_id, col_time_id])[col_wap].apply(get_log_return)\n",
    "    vol_wap = vol_wap.fillna(0)\n",
    "    return vol_wap\n",
    "\n",
    "\n",
    "def get_bid_ask_spread(df_book, col_bid_price1, col_ask_price1, col_bid_price2, col_ask_price2):\n",
    "    \"\"\"\n",
    "    Get Combined bid ask spread using both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    bas_numerator   = df_book[[col_ask_price1, col_ask_price2]].min(axis=1)\n",
    "    bas_denominator = df_book[[col_bid_price1, col_bid_price2]].max(axis=1) - 1\n",
    "    \n",
    "    return bas_numerator / bas_denominator\n",
    "\n",
    "\n",
    "def get_vertical_spread(df_book, col_price1, col_price2):\n",
    "    \"\"\"\n",
    "    Returns the vertical spread for Bid/Ask price features inputted.\n",
    "    \"\"\"\n",
    "    v_spread = df_book[col_price1] - df_book[col_price2]\n",
    "    return v_spread\n",
    "\n",
    "\n",
    "def get_spread_feature(df_book, col_price_a, col_price_b):\n",
    "    \"\"\"\n",
    "    Returns a spread feature based on the price features inputted.\n",
    "    \"\"\"\n",
    "    spread_feat = df_book[col_price_a] - df_book[col_price_b]\n",
    "    return spread_feat\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    \"\"\"\n",
    "    Returns the realized volatility for a given period.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the Root Mean Squared Prediction Error.\n",
    "    \"\"\"\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def get_row_id(df, col_stock_id, col_time_id):\n",
    "    \"\"\"\n",
    "    Returns row ids in format required for submission. \n",
    "    \"\"\"\n",
    "    row_ids = df[col_stock_id].astype(\"str\") + \"-\" + df[col_time_id].astype(\"str\")\n",
    "    return row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bda53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile data manipulation helper functions into complete functions\n",
    "def extract_trade_feature_set(df_trade):\n",
    "    \"\"\"\n",
    "    Returns engineered trade dataset, where each row is a unique stock ID/time ID pair.\n",
    "    \"\"\"\n",
    "    # Get the Log return for trades by stock ID and time ID\n",
    "    df_trade[\"trade_log_return\"] = get_trade_log_return(df_trade, \"stock_id\", \"time_id\", \"price\")\n",
    "    \n",
    "    # Get aggregate statistics for specified numerical features\n",
    "    trade_features = [\"price\", \"size\", \"order_count\", \"trade_log_return\"]\n",
    "    \n",
    "    for trade_feature in trade_features:\n",
    "        # Get min aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"min\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get max aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"max\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get mean aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"mean\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get std aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"std\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get sum aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"sum\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "    \n",
    "    # Reduce trade df to just unique stock ID and time ID pairs\n",
    "    df_trade = df_trade.drop([\"seconds_in_bucket\", \"price\", \"size\", \"order_count\", \"trade_log_return\"], axis=1)\n",
    "    df_trade = df_trade.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_trade\n",
    "\n",
    "\n",
    "def extract_book_feature_set(df_book):\n",
    "    \"\"\"\n",
    "    Returns engineered book dataset, where each row is a unique stock ID/time ID pair.\n",
    "    \"\"\"\n",
    "    # WAP for both bid/ask price/size features\n",
    "    df_book[\"wap1\"] = get_wap(df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\")\n",
    "    df_book[\"wap2\"] = get_wap(df_book, \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\")\n",
    "    # Combined WAP\n",
    "    df_book[\"wap_combined\"] = get_wap_combined(\n",
    "        df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\", \n",
    "        \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\"\n",
    "    )\n",
    "    # Average WAP for both bid/ask price/size features\n",
    "    df_book[\"wap_avg\"] = get_wap_avg(\n",
    "        df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\", \n",
    "        \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\"\n",
    "    )\n",
    "    \n",
    "    # Get VWAPS based on different WAP features\n",
    "    df_book[\"vol_wap1\"]         = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap1\")\n",
    "    df_book[\"vol_wap2\"]         = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap2\")\n",
    "    df_book[\"vol_wap_combined\"] = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap_combined\")\n",
    "    df_book[\"vol_wap_avg\"]      = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap_avg\")\n",
    "    \n",
    "    # Get different spread features\n",
    "    df_book[\"bid_ask_spread\"] = get_bid_ask_spread(df_book, \"bid_price1\", \"ask_price1\", \"bid_price2\",\"ask_price2\")\n",
    "    df_book[\"bid_v_spread\"]   = get_vertical_spread(df_book, \"bid_price1\", \"bid_price2\")\n",
    "    df_book[\"ask_v_spread\"]   = get_vertical_spread(df_book, \"ask_price1\", \"ask_price2\")\n",
    "    df_book[\"h_spread1\"]      = get_spread_feature(df_book, \"ask_price1\", \"bid_price1\")\n",
    "    df_book[\"h_spread2\"]      = get_spread_feature(df_book, \"ask_price2\", \"bid_price2\")\n",
    "    df_book[\"spread_diff1\"]   = get_spread_feature(df_book, \"ask_price1\", \"bid_price2\")\n",
    "    df_book[\"spread_diff2\"]   = get_spread_feature(df_book, \"ask_price2\", \"bid_price1\")\n",
    "    \n",
    "    # Get aggregated volatility features for each VWAP\n",
    "    vol_features = [\"vol_wap1\", \"vol_wap2\", \"vol_wap_combined\", \"vol_wap_avg\"]\n",
    "    \n",
    "    for vol_feature in vol_features:\n",
    "         df_book = df_book.merge(\n",
    "             get_agg_feature(df=df_book, col_name=vol_feature, func=realized_volatility),\n",
    "             how=\"left\",\n",
    "             on=[\"stock_id\", \"time_id\"]\n",
    "         )\n",
    "            \n",
    "    # Get aggregated features for different spread features\n",
    "    spread_features = [\n",
    "        \"bid_ask_spread\", \"bid_v_spread\", \"ask_v_spread\", \"h_spread1\", \n",
    "        \"h_spread2\", \"spread_diff1\", \"spread_diff2\"\n",
    "    ]\n",
    "    \n",
    "    for spread_feature in spread_features:\n",
    "        # Get min aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"min\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get max aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"max\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get mean aggregations\n",
    "        df_book = df_book.merge(\n",
    "             get_agg_feature(df=df_book, col_name=spread_feature, func=\"mean\"),\n",
    "             how=\"left\",\n",
    "             on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get std aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"std\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get sum aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"sum\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "\n",
    "    # Reduce trade df to just unique stock ID and time ID pairs\n",
    "    df_book = df_book.drop([\n",
    "        \"seconds_in_bucket\", \"bid_price1\", \"ask_price1\", \"bid_price2\", \n",
    "        \"ask_price2\", \"bid_size1\", \"ask_size1\", \"bid_size2\", \"ask_size2\",\n",
    "        # WAP features\n",
    "        \"wap1\", \"wap2\", \"wap_combined\", \"wap_avg\", \"vol_wap1\", \n",
    "        \"vol_wap2\", \"vol_wap_combined\", \"vol_wap_avg\", \n",
    "        # Spread features\n",
    "        \"bid_ask_spread\", \"bid_v_spread\", \"ask_v_spread\", \"h_spread1\", \n",
    "        \"h_spread2\", \"spread_diff1\", \"spread_diff2\" \n",
    "    ], axis=1)\n",
    "    df_book = df_book.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_book\n",
    "\n",
    "\n",
    "def get_initial_feature_set(df_train, df_trade, df_book):\n",
    "    \"\"\"\n",
    "    Returns engineered feature set with labels, before preprocessing\n",
    "    \"\"\"\n",
    "    # Extract trade and book features\n",
    "    df_trade = extract_trade_feature_set(df_trade)\n",
    "    df_book  = extract_book_feature_set(df_book)\n",
    "    # Merge trade and book features to labels\n",
    "    df_train = pd.merge(df_train, df_trade, how=\"inner\", on=[\"stock_id\", \"time_id\"])\n",
    "    df_train = pd.merge(df_train, df_book, how=\"inner\", on=[\"stock_id\", \"time_id\"])\n",
    "    \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271b102",
   "metadata": {},
   "source": [
    "### Full Data Manipulation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b8794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_initial_feature_set(train_labels, train_trade, train_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "607b695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key parameters\n",
    "SEED = 19\n",
    "np.random.seed(SEED)\n",
    "\n",
    "SCALER_METHOD = RobustScaler()\n",
    "\n",
    "FEATURE_SELECTOR = RandomForestRegressor(random_state=SEED)\n",
    "NUM_FEATURES = 500\n",
    "\n",
    "NUM_COMPONENTS = 200\n",
    "PCA_METHOD = PCA(n_components=NUM_COMPONENTS, random_state=SEED)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "KFOLDS = 2\n",
    "PATIENCE = 10\n",
    "\n",
    "MODEL_TO_USE = 'nn'\n",
    "model_name_save = MODEL_TO_USE + '_final_classifier_seed' + str(SEED)\n",
    "\n",
    "print(f'Model name: {model_name_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e17a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define full dataset transformation pipeline\n",
    "def transform_dataset(train_df, val_df,\n",
    "                      verbose=0, \n",
    "                      scaler=SCALER_METHOD, \n",
    "                      feature_selector=FEATURE_SELECTOR,\n",
    "                      num_features=NUM_FEATURES,\n",
    "                      pca=PCA_METHOD, \n",
    "                      seed=SEED\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Takes in train and validation datasets, and applies feature transformations,\n",
    "    feature selection, scaling and pca (dependent on arguments). \n",
    "    \n",
    "    Returns transformed X_train and X_val data ready for training/prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ## DATA PREPARATION ##\n",
    "\n",
    "    # Drop unique ID feature\n",
    "    X_train = train_df.drop(\"target\", axis=1)\n",
    "    X_val   = val_df.drop(\"target\", axis=1)\n",
    "\n",
    "    # Get indices for train and validation dfs - we'll need these later\n",
    "    train_idx = list(X_train.index)\n",
    "    val_idx   = list(X_val.index)\n",
    "       \n",
    "    # Get train colnames before scaling and feature selection\n",
    "    feat_cols = X_train.drop([\"stock_id\", \"time_id\"], axis=1).columns\n",
    "    id_cols   = \n",
    "    \n",
    "    ## SCALING ##\n",
    "    \n",
    "    if scaler != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING SCALER...')\n",
    "            \n",
    "        # Fit and transform scaler to train and val\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_val  = scaler.transform(X_train)\n",
    "        # Convert to back dataframe\n",
    "        X_train = pd.DataFrame(X_train, index=train_idx, columns=colnames)\n",
    "        X_val   = pd.DataFrame(X_val, index=val_idx, columns=colnames)\n",
    "        \n",
    "        \n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49152f73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (999, 61), indices imply (999, 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1670\u001b[0m                 blocks = [\n\u001b[0;32m-> 1671\u001b[0;31m                     \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1672\u001b[0m                 ]\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[1;32m   2743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2744\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    130\u001b[0m             raise ValueError(\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 61, placement implies 59",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8b5f1bd04012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1999\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-8a50c7e624d9>\u001b[0m in \u001b[0;36mtransform_dataset\u001b[0;34m(train_df, val_df, verbose, scaler, feature_selector, num_features, pca, seed)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mX_val\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Convert to back dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mX_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1679\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"values\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (999, 61), indices imply (999, 59)"
     ]
    }
   ],
   "source": [
    "transform_dataset(train[0:999], train[1000:1999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7920136f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c7115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051759e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c6c83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb1902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4eb15c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df964dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77f45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775d0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b405b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_feature_set(X_train, X_test, y_train, y_test, \n",
    "                          verbose=0, \n",
    "                          scaler=SCALER_METHOD, \n",
    "                          feature_selector=FEATURE_SELECTOR,\n",
    "                          num_features=NUM_FEATURES,\n",
    "                          pca=PCA_METHOD, \n",
    "                          seed=SEED):\n",
    "    \"\"\"\n",
    "    Takes in X_train and X_test datasets, and applies feature transformations,\n",
    "    feature selection, scaling and pca (dependent on arguments). \n",
    "    \n",
    "    Returns transformed X_train and X_test data ready for training/prediction, and returns\n",
    "    list of numerical cols and categorical cols, for the use of creating embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    ## SCALING ##\n",
    "    \n",
    "    if scaler != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING SCALER...')\n",
    "            \n",
    "        # Fit and transform scaler to train and val\n",
    "        scaler.fit(X_train_numerical)\n",
    "        X_train_numerical = scaler.transform(X_train_numerical)\n",
    "        X_test_numerical  = scaler.transform(X_test_numerical)\n",
    "        # Convert to back dataframe\n",
    "        X_train_numerical = pd.DataFrame(X_train_numerical, index=train_idx, columns=num_cols)\n",
    "        X_test_numerical  = pd.DataFrame(X_test_numerical, index=test_idx, columns=num_cols)\n",
    "    \n",
    "    \n",
    "    ## FEATURE SELECTION ##\n",
    "    \n",
    "    # Feature selection is only ran on numerical data\n",
    "    if feature_selector != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING FEATURE SELECTOR...')\n",
    "        num_cols = X_train_numerical.shape[1]\n",
    "            \n",
    "        # Fit tree based classifier to select features\n",
    "        selected_features = [] \n",
    "        labels = list(y_train.columns)\n",
    "        \n",
    "        # Run feature selection for each label and record the selected feature names\n",
    "        for label in labels:\n",
    "            # Fit feature selection model\n",
    "            feature_selector_fit = SelectFromModel(estimator=feature_selector)\n",
    "            feature_selector_fit = feature_selector_fit.fit(X_train_numerical, y_train[label])\n",
    "            \n",
    "            # Retrieve the names of the features selected for each label\n",
    "            feature_idx = feature_selector_fit.get_support()\n",
    "            feature_select = list(X_train_numerical.columns[feature_idx])\n",
    "            selected_features.append(feature_select)\n",
    "        \n",
    "        # Count numbers of times features were selected\n",
    "        selected_features = [feature for sublist in selected_features for feature in sublist]\n",
    "        selected_features = pd.Series(selected_features).value_counts()\n",
    "        selected_features = selected_features.sort_values(ascending=False).reset_index()\n",
    "        # Select top n features, based on num_features\n",
    "        selected_features = list(selected_features[:num_features].rename(columns={'index':'feature'})['feature'])\n",
    "        # Subset datasets to selected features only\n",
    "        X_train_numerical = X_train_numerical[selected_features]\n",
    "        X_test_numerical  = X_test_numerical[selected_features]\n",
    "        # Store column names for selected features\n",
    "        selected_features = [selected_features, cat_cols]\n",
    "        selected_features = [item for sublist in selected_features for item in sublist]\n",
    "        \n",
    "        if verbose == 1: \n",
    "            print(f'{num_cols - X_train_numerical.shape[1]} features removed in feature selection.')\n",
    "            del num_cols\n",
    "        \n",
    "            \n",
    "    ## PCA ##\n",
    "    \n",
    "    if pca != None:\n",
    "        if verbose == 1:\n",
    "            print('APPLYING PCA...')\n",
    "        # Fit and transform pca to train and val\n",
    "        pca.fit(X_train_numerical)\n",
    "        X_train_numerical = pca.transform(X_train_numerical)\n",
    "        X_test_numerical  = pca.transform(X_test_numerical)\n",
    "        if verbose == 1:\n",
    "            print(f'NUMBER OF PRINCIPAL COMPONENTS: {pca.n_components_}')\n",
    "        # Convert numerical features into pandas dataframe and clean colnames\n",
    "        X_train_numerical = pd.DataFrame(X_train_numerical, index=train_idx).add_prefix('pca_')\n",
    "        X_test_numerical  = pd.DataFrame(X_test_numerical, index=test_idx).add_prefix('pca_')\n",
    "    \n",
    "    \n",
    "    ## CATEGORICAL FEATURES ##\n",
    "    \n",
    "    # Get categorical and numerical column names\n",
    "    num_cols = X_train_numerical.columns\n",
    "    cat_cols = X_train_categorical.columns\n",
    "\n",
    "    # Encode categorical features\n",
    "    X_train_categorical = X_train_categorical.apply(lambda x: x.cat.codes)\n",
    "    X_test_categorical  = X_test_categorical.apply(lambda x: x.cat.codes)\n",
    "\n",
    "    # Concatenate transformed categorical features with transformed numerical features  \n",
    "    X_train = pd.concat([X_train_categorical, X_train_numerical], axis=1)\n",
    "    X_test  = pd.concat([X_test_categorical, X_test_numerical], axis=1)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(f'TRAIN SHAPE: \\t\\t{X_train.shape}')\n",
    "        print(f'VALIDATION SHAPE: \\t{X_test.shape}')\n",
    "    \n",
    "    return X_train, X_test, num_cols, cat_cols, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c616b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabdb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e62ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab3e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fb9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6912414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6b9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c39fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
