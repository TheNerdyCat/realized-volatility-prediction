{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7e675d",
   "metadata": {},
   "source": [
    "# Optiver Realized Volatility Prediction - Train\n",
    "\n",
    "**This notebook seeks to EDITS HERE**\n",
    "---------\n",
    "\n",
    "## Files\n",
    "**book_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n",
    "\n",
    " - `stock_id` - ID code for the stock. Not all `stock_id`s exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n",
    " - `time_id` - ID code for the time bucket. `time_id`s are not necessarily sequential but are consistent across all stocks.\n",
    " - `seconds_in_bucket` - Number of seconds from the start of the bucket, always starting from 0.\n",
    " - `bid_price[1/2]` - Normalized prices of the most/second most competitive buy level.\n",
    " - `ask_price[1/2]` - Normalized prices of the most/second most competitive sell level.\n",
    " - `bid_size[1/2]` - The number of shares on the most/second most competitive buy level.\n",
    " - `ask_size[1/2]` - The number of shares on the most/second most competitive sell level.\n",
    " \n",
    "**trade_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `seconds_in_bucket` - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n",
    " - `price` - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n",
    " - `size` - The sum number of shares traded.\n",
    " - `order_count` - The number of unique trade orders taking place.\n",
    " \n",
    "**train.csv** The ground truth values for the training set.\n",
    "\n",
    " - `stock_id` - Same as above, but since this is a csv the column will load as an integer instead of categorical.\n",
    " - `time_id` - Same as above.\n",
    " - `target` - The realized volatility computed over the 10 minute window following the feature data under the same `stock_id`/`time_id`. There is no overlap between feature and target data. \n",
    " \n",
    "**test.csv** Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `row_id` - Unique identifier for the submission row. There is one row for each existing `stock_id`/`time_id` pair. Each time window is not necessarily containing every individual stock.\n",
    " \n",
    "**sample_submission.csv** - A sample submission file in the correct format.\n",
    "\n",
    " - `row_id` - Same as in test.csv.\n",
    " - `target` - Same definition as in **train.csv**. The benchmark is using the median target value from **train.csv**.\n",
    " \n",
    "## Prepare Environment\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcab26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq # To handle parquet files\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data prep\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# Model cross validation and evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de666dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 1\n",
      "Num CPU Threads Available: 64\n",
      "REPLICAS: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "\n",
    "# Get number of cpu cores for multiprocessing\n",
    "try:\n",
    "    cpus = int(multiprocessing.cpu_count() / 2)\n",
    "except NotImplementedError:\n",
    "    cpus = 1 # Default number of cores\n",
    "    \n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(f\"Num CPU Threads Available: {cpus}\")\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e027214",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ebc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "comp_dir_path = Path(\"../input/optiver-realized-volatility-prediction\")\n",
    "\n",
    "# Train paths\n",
    "train_book_path   = comp_dir_path/\"book_train.parquet\"\n",
    "train_trade_path  = comp_dir_path/\"trade_train.parquet\"\n",
    "train_labels_path = comp_dir_path/\"train.csv\"\n",
    "\n",
    "# Test paths\n",
    "test_book_path   = comp_dir_path/\"book_test.parquet\"\n",
    "test_trade_path  = comp_dir_path/\"trade_test.parquet\"\n",
    "test_labels_path = comp_dir_path/\"test.csv\"\n",
    "\n",
    "# Sample submission path\n",
    "sample_sub_path = comp_dir_path/\"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d99a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for data reading\n",
    "def get_stock_ids_list(data_dir_path):\n",
    "    data_dir = os.listdir(data_dir_path)\n",
    "    # Get list of stock ids in directory\n",
    "    stock_ids = list(map(lambda x: x.split(\"=\")[1], data_dir))\n",
    "    return stock_ids\n",
    "    \n",
    "    \n",
    "def load_book_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        book_stock_id_path = os.path.join(train_book_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        book_stock_id_path = os.path.join(test_book_path, stock_id_ext)\n",
    "    book_stock_id = pd.read_parquet(book_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    book_stock_id[\"stock_id\"] = stock_id\n",
    "            \n",
    "    return book_stock_id\n",
    "\n",
    "def load_trade_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        trade_stock_id_path = os.path.join(train_trade_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        trade_stock_id_path = os.path.join(test_trade_path, stock_id_ext)\n",
    "    trade_stock_id = pd.read_parquet(trade_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    trade_stock_id[\"stock_id\"] = stock_id\n",
    "            \n",
    "    return trade_stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394aedc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATA DIMENSIONS\n",
      "train_book shape: (2425085, 11)\n",
      "train_trade shape: (419653, 6)\n",
      "train_labels shape: (428932, 3)\n",
      "\n",
      "TEST DATA DIMENSIONS\n",
      "test_book shape: (3, 11)\n",
      "test_trade shape: (3, 6)\n",
      "test_labels shape: (3, 3)\n",
      "\n",
      "CPU times: user 226 ms, sys: 766 ms, total: 993 ms\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get list of stock ids\n",
    "train_stock_ids = get_stock_ids_list(train_book_path)\n",
    "test_stock_ids = get_stock_ids_list(test_book_path)\n",
    "\n",
    "# Read train data\n",
    "is_train_test = \"train\"\n",
    "# Create worker pool and read\n",
    "pool         = multiprocessing.Pool(processes=cpus)\n",
    "train_book   = pd.concat(pool.map(load_book_stock_id_data, train_stock_ids[0:2]))\n",
    "train_trade  = pd.concat(pool.map(load_trade_stock_id_data, train_stock_ids[0:2]))\n",
    "train_labels = pd.read_csv(train_labels_path)\n",
    "# Close worker pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Read test data\n",
    "is_train_test = \"test\"\n",
    "# Create worker pool and read\n",
    "pool        = multiprocessing.Pool(processes=cpus)\n",
    "test_book   = pd.concat(pool.map(load_book_stock_id_data, test_stock_ids))\n",
    "test_trade  = pd.concat(pool.map(load_trade_stock_id_data, test_stock_ids))\n",
    "test_labels = pd.read_csv(test_labels_path)\n",
    "\n",
    "# Read sample submission\n",
    "sample_sub = pd.read_csv(sample_sub_path)\n",
    "\n",
    "# Print data dimensions\n",
    "print(\"TRAIN DATA DIMENSIONS\")\n",
    "print(f\"train_book shape: {train_book.shape}\")\n",
    "print(f\"train_trade shape: {train_trade.shape}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}\")\n",
    "\n",
    "print(\"\\nTEST DATA DIMENSIONS\")\n",
    "print(f\"test_book shape: {test_book.shape}\")\n",
    "print(f\"test_trade shape: {test_trade.shape}\")\n",
    "print(f\"test_labels shape: {test_labels.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7b7fa",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f51566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for data manipulation\n",
    "def get_log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def get_trade_log_return(df_trade, col_stock_id, col_time_id, col_price):\n",
    "    \"\"\"\n",
    "    Returns the Log Return at each time ID.\n",
    "    \"\"\"\n",
    "    trade_log_return = df_trade.groupby([col_stock_id, col_time_id])[col_price].apply(get_log_return)\n",
    "    trade_log_return = trade_log_return.fillna(0)\n",
    "    return trade_log_return\n",
    "\n",
    "\n",
    "def get_agg_feature(df, col_name, func):\n",
    "    \"\"\"\n",
    "    Returns aggregated feature by stock ID and time ID based on input df and feature.\n",
    "    \"\"\"\n",
    "    if \"function\" in str(func):\n",
    "        func_str = str(func).split(\" \")[1]\n",
    "        agg_feat_col_name = f\"{col_name}_{func_str}\"\n",
    "    else:\n",
    "        agg_feat_col_name = f\"{col_name}_{func}\"\n",
    "    \n",
    "    agg_feat = df.groupby(by=[\"stock_id\", \"time_id\"])[col_name].agg(func)\n",
    "    agg_feat = agg_feat.reset_index().rename(columns={col_name: agg_feat_col_name})\n",
    "    \n",
    "    return agg_feat\n",
    "\n",
    "\n",
    "def get_wap(df_book, col_bid_price, col_ask_price, col_bid_size, col_ask_size):\n",
    "    \"\"\"\n",
    "    Returns Weighted Average Price. \n",
    "    \"\"\"\n",
    "    wap_numerator = df_book[col_bid_price]  * df_book[col_ask_size]\n",
    "    wap_numerator += df_book[col_ask_price] * df_book[col_bid_size]\n",
    "    \n",
    "    wap_denominator = df_book[col_bid_size] + df_book[col_ask_size]\n",
    "    \n",
    "    return wap_numerator / wap_denominator\n",
    "\n",
    "\n",
    "def get_wap_combined(df_book, col_bid_price1, col_ask_price1, col_bid_size1, col_ask_size1,\n",
    "                     col_bid_price2, col_ask_price2, col_bid_size2, col_ask_size2):    \n",
    "    \"\"\"\n",
    "    Returns the Combined Weighted Average Price for both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    wap_numerator1  = df_book[col_bid_price1] * df_book[col_ask_size1]\n",
    "    wap_numerator1 += df_book[col_ask_price1] * df_book[col_bid_size1]\n",
    "    wap_numerator2  = df_book[col_bid_price2] * df_book[col_ask_size2]\n",
    "    wap_numerator2 += df_book[col_ask_price2] * df_book[col_bid_size2]\n",
    "    \n",
    "    wap_denominator  = df_book[col_bid_size1] + df_book[col_ask_size1]\n",
    "    wap_denominator += df_book[col_bid_size2] + df_book[col_ask_size2]\n",
    "    \n",
    "    return (wap_numerator1 + wap_numerator2) / wap_denominator\n",
    "\n",
    "\n",
    "def get_wap_avg(df_book, col_bid_price1, col_ask_price1, col_bid_size1, col_ask_size1,\n",
    "                col_bid_price2, col_ask_price2, col_bid_size2, col_ask_size2):\n",
    "    \"\"\"\n",
    "    Returns the Combined Average Weighted Average Price for both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    wap_numerator1  = df_book[col_bid_price1] * df_book[col_ask_size1]\n",
    "    wap_numerator1 += df_book[col_ask_price1] * df_book[col_bid_size1]\n",
    "    wap_numerator1 /= df_book[col_bid_size1] + df_book[col_ask_size1]\n",
    "    \n",
    "    wap_numerator2  = df_book[col_bid_price2] * df_book[col_ask_size2]\n",
    "    wap_numerator2 += df_book[col_ask_price2] * df_book[col_bid_size2]\n",
    "    wap_numerator2 /= df_book[col_bid_size2] + df_book[col_ask_size2]\n",
    "    \n",
    "    return (wap_numerator1 + wap_numerator2) / 2\n",
    "\n",
    "\n",
    "def get_vol_wap(df_book, col_stock_id, col_time_id, col_wap):\n",
    "    \"\"\"\n",
    "    Returns the Volume Weighted Average Price at each time ID.\n",
    "    \"\"\"\n",
    "    vol_wap = df_book.groupby([col_stock_id, col_time_id])[col_wap].apply(get_log_return)\n",
    "    vol_wap = vol_wap.fillna(0)\n",
    "    return vol_wap\n",
    "\n",
    "\n",
    "def get_bid_ask_spread(df_book, col_bid_price1, col_ask_price1, col_bid_price2, col_ask_price2):\n",
    "    \"\"\"\n",
    "    Get Combined bid ask spread using both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    bas_numerator   = df_book[[col_ask_price1, col_ask_price2]].min(axis=1)\n",
    "    bas_denominator = df_book[[col_bid_price1, col_bid_price2]].max(axis=1) - 1\n",
    "    \n",
    "    return bas_numerator / bas_denominator\n",
    "\n",
    "\n",
    "def get_vertical_spread(df_book, col_price1, col_price2):\n",
    "    \"\"\"\n",
    "    Returns the vertical spread for Bid/Ask price features inputted.\n",
    "    \"\"\"\n",
    "    v_spread = df_book[col_price1] - df_book[col_price2]\n",
    "    return v_spread\n",
    "\n",
    "\n",
    "def get_spread_feature(df_book, col_price_a, col_price_b):\n",
    "    \"\"\"\n",
    "    Returns a spread feature based on the price features inputted.\n",
    "    \"\"\"\n",
    "    spread_feat = df_book[col_price_a] - df_book[col_price_b]\n",
    "    return spread_feat\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    \"\"\"\n",
    "    Returns the realized volatility for a given period.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the Root Mean Squared Prediction Error.\n",
    "    \"\"\"\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def get_row_id(df, col_stock_id, col_time_id):\n",
    "    \"\"\"\n",
    "    Returns row ids in format required for submission. \n",
    "    \"\"\"\n",
    "    row_ids = df[col_stock_id].astype(\"str\") + \"-\" + df[col_time_id].astype(\"str\")\n",
    "    return row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bda53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile data manipulation helper functions into complete functions\n",
    "def extract_trade_feature_set(df_trade):\n",
    "    \"\"\"\n",
    "    Returns engineered trade dataset, where each row is a unique stock ID/time ID pair.\n",
    "    \"\"\"\n",
    "    # Get the Log return for trades by stock ID and time ID\n",
    "    df_trade[\"trade_log_return\"] = get_trade_log_return(df_trade, \"stock_id\", \"time_id\", \"price\")\n",
    "    #df_trade[\"trade_log_return\"] = df_trade[\"trade_log_return\"].fillna(0)\n",
    "    \n",
    "    # Get aggregate statistics for specified numerical features\n",
    "    trade_features = [\"price\", \"size\", \"order_count\", \"trade_log_return\"]\n",
    "    \n",
    "    for trade_feature in trade_features:\n",
    "        # Get min aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"min\", rename=True),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get max aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"max\", rename=True),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get mean aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"mean\", rename=True),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get std aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"std\", rename=True),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get sum aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"sum\", rename=True),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "    \n",
    "    # Reduce trade df to just unique stock ID and time ID pairs\n",
    "    df_trade = df_trade.drop([\"seconds_in_bucket\", \"price\", \"size\", \"order_count\", \"trade_log_return\"], axis=1)\n",
    "    df_trade = df_trade.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_trade\n",
    "\n",
    "\n",
    "def extract_book_feature_set(df_book):\n",
    "    \"\"\"\n",
    "    Returns engineered book dataset, where each row is a unique stock ID/time ID pair.\n",
    "    \"\"\"\n",
    "    # WAP for both bid/ask price/size features\n",
    "    df_book[\"wap1\"] = get_wap(df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\")\n",
    "    df_book[\"wap2\"] = get_wap(df_book, \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\")\n",
    "    # Combined WAP\n",
    "    df_book[\"wap_combined\"] = get_wap_combined(\n",
    "        df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\", \n",
    "        \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\"\n",
    "    )\n",
    "    # Average WAP for both bid/ask price/size features\n",
    "    df_book[\"wap_avg\"] = get_wap_avg(\n",
    "        df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\", \n",
    "        \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\"\n",
    "    )\n",
    "    \n",
    "    # Get VWAPS based on different WAP features\n",
    "    df_book[\"vol_wap1\"]         = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap1\")\n",
    "    df_book[\"vol_wap2\"]         = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap2\")\n",
    "    df_book[\"vol_wap_combined\"] = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap_combined\")\n",
    "    df_book[\"vol_wap_avg\"]      = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap_avg\")\n",
    "    \n",
    "    # Get different spread features\n",
    "    df_book[\"bid_ask_spread\"] = get_bid_ask_spread(df_book, \"bid_price1\", \"ask_price1\", \"bid_price2\",\"ask_price2\")\n",
    "    df_book[\"bid_v_spread\"]   = get_vertical_spread(df_book, \"bid_price1\", \"bid_price2\")\n",
    "    df_book[\"ask_v_spread\"]   = get_vertical_spread(df_book, \"ask_price1\", \"ask_price2\")\n",
    "    df_book[\"h_spread1\"]      = get_spread_feature(df_book, \"ask_price1\", \"bid_price1\")\n",
    "    df_book[\"h_spread2\"]      = get_spread_feature(df_book, \"ask_price2\", \"bid_price2\")\n",
    "    df_book[\"spread_diff1\"]   = get_spread_feature(df_book, \"ask_price1\", \"bid_price2\")\n",
    "    df_book[\"spread_diff2\"]   = get_spread_feature(df_book, \"ask_price2\", \"bid_price1\")\n",
    "    \n",
    "    # Get aggregated volatility features for each VWAP\n",
    "    vol_features = [\"vol_wap1\", \"vol_wap2\", \"vol_wap_combined\", \"vol_wap_avg\"]\n",
    "    \n",
    "    for vol_feature in vol_features:\n",
    "         df_book = df_book.merge(\n",
    "             get_agg_feature(df=df_book, col_name=vol_feature, func=realized_volatility),\n",
    "             how=\"left\",\n",
    "             on=[\"stock_id\", \"time_id\"]\n",
    "         )\n",
    "            \n",
    "    # Get aggregated features for different spread features\n",
    "    spread_features = [\n",
    "        \"bid_ask_spread\", \"bid_v_spread\", \"ask_v_spread\", \"h_spread1\", \n",
    "        \"h_spread2\", \"spread_diff1\", \"spread_diff2\"\n",
    "    ]\n",
    "    \n",
    "    for spread_feature in spread_features:\n",
    "        # Get min aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"min\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get max aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"max\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get mean aggregations\n",
    "        df_book = df_book.merge(\n",
    "             get_agg_feature(df=df_book, col_name=spread_feature, func=\"mean\"),\n",
    "             how=\"left\",\n",
    "             on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get std aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"std\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get sum aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"sum\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "\n",
    "    # Reduce trade df to just unique stock ID and time ID pairs\n",
    "    df_book = df_book.drop([\n",
    "        \"seconds_in_bucket\", \"bid_price1\", \"ask_price1\", \"bid_price2\", \n",
    "        \"ask_price2\", \"bid_size1\", \"ask_size1\", \"bid_size2\", \"ask_size2\",\n",
    "        # WAP features\n",
    "        \"wap1\", \"wap2\", \"wap_combined\", \"wap_avg\", \"vol_wap1\", \n",
    "        \"vol_wap2\", \"vol_wap_combined\", \"vol_wap_avg\", \n",
    "        # Spread features\n",
    "        \"bid_ask_spread\", \"bid_v_spread\", \"ask_v_spread\", \"h_spread1\", \n",
    "        \"h_spread2\", \"spread_diff1\", \"spread_diff2\" \n",
    "    ], axis=1)\n",
    "    df_book = df_book.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0d495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_id</th>\n",
       "      <th>stock_id</th>\n",
       "      <th>vol_wap1_realized_volatility</th>\n",
       "      <th>vol_wap2_realized_volatility</th>\n",
       "      <th>vol_wap_combined_realized_volatility</th>\n",
       "      <th>vol_wap_avg_realized_volatility</th>\n",
       "      <th>bid_ask_spread_min</th>\n",
       "      <th>bid_ask_spread_max</th>\n",
       "      <th>bid_ask_spread_mean</th>\n",
       "      <th>bid_ask_spread_std</th>\n",
       "      <th>...</th>\n",
       "      <th>spread_diff1_min</th>\n",
       "      <th>spread_diff1_max</th>\n",
       "      <th>spread_diff1_mean</th>\n",
       "      <th>spread_diff1_std</th>\n",
       "      <th>spread_diff1_sum</th>\n",
       "      <th>spread_diff2_min</th>\n",
       "      <th>spread_diff2_max</th>\n",
       "      <th>spread_diff2_mean</th>\n",
       "      <th>spread_diff2_std</th>\n",
       "      <th>spread_diff2_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.006999</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>235.541718</td>\n",
       "      <td>7.051342e+02</td>\n",
       "      <td>320.050354</td>\n",
       "      <td>99.705377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.311224</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.303775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>-39871.871094</td>\n",
       "      <td>3.978736e+04</td>\n",
       "      <td>1878.224854</td>\n",
       "      <td>17613.091377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.107207</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.105850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>-13928.000000</td>\n",
       "      <td>8.362587e+03</td>\n",
       "      <td>-1408.515747</td>\n",
       "      <td>3016.430184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.173237</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.173524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>-5406.040039</td>\n",
       "      <td>-3.786276e+02</td>\n",
       "      <td>-1293.191162</td>\n",
       "      <td>1691.972493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.125890</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.116127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001894</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>-4768.609375</td>\n",
       "      <td>-1.045971e+03</td>\n",
       "      <td>-2068.633545</td>\n",
       "      <td>1141.670493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.103437</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.089079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7655</th>\n",
       "      <td>32751</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-14897.615234</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.231676</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.219720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7656</th>\n",
       "      <td>32753</td>\n",
       "      <td>1</td>\n",
       "      <td>0.010829</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.010422</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>-14362.764648</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>0.538242</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.521524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7657</th>\n",
       "      <td>32758</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>-11208.745117</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.246919</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.244062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7658</th>\n",
       "      <td>32763</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>0.005773</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>-16265.990234</td>\n",
       "      <td>1.626199e+04</td>\n",
       "      <td>7.209878</td>\n",
       "      <td>2403.935707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.265700</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.283659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659</th>\n",
       "      <td>32767</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>-10113.757812</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.122611</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.122609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7660 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      time_id stock_id  vol_wap1_realized_volatility  \\\n",
       "0           5        0                      0.004499   \n",
       "1          11        0                      0.001204   \n",
       "2          16        0                      0.002369   \n",
       "3          31        0                      0.002574   \n",
       "4          62        0                      0.001894   \n",
       "...       ...      ...                           ...   \n",
       "7655    32751        1                      0.003723   \n",
       "7656    32753        1                      0.010829   \n",
       "7657    32758        1                      0.003135   \n",
       "7658    32763        1                      0.003750   \n",
       "7659    32767        1                      0.001681   \n",
       "\n",
       "      vol_wap2_realized_volatility  vol_wap_combined_realized_volatility  \\\n",
       "0                         0.006999                              0.004106   \n",
       "1                         0.002476                              0.001507   \n",
       "2                         0.004801                              0.002469   \n",
       "3                         0.003637                              0.002709   \n",
       "4                         0.003257                              0.001932   \n",
       "...                            ...                                   ...   \n",
       "7655                      0.004996                              0.003322   \n",
       "7656                      0.012168                              0.010422   \n",
       "7657                      0.004268                              0.002797   \n",
       "7658                      0.005773                              0.003814   \n",
       "7659                      0.002399                              0.001716   \n",
       "\n",
       "      vol_wap_avg_realized_volatility  bid_ask_spread_min  bid_ask_spread_max  \\\n",
       "0                            0.004115          235.541718        7.051342e+02   \n",
       "1                            0.001268       -39871.871094        3.978736e+04   \n",
       "2                            0.002719       -13928.000000        8.362587e+03   \n",
       "3                            0.002625        -5406.040039       -3.786276e+02   \n",
       "4                            0.001901        -4768.609375       -1.045971e+03   \n",
       "...                               ...                 ...                 ...   \n",
       "7655                         0.003101       -14897.615234                 inf   \n",
       "7656                         0.009786       -14362.764648                 inf   \n",
       "7657                         0.002765       -11208.745117                 inf   \n",
       "7658                         0.003890       -16265.990234        1.626199e+04   \n",
       "7659                         0.001732       -10113.757812                 inf   \n",
       "\n",
       "      bid_ask_spread_mean  bid_ask_spread_std  ...  spread_diff1_min  \\\n",
       "0              320.050354           99.705377  ...          0.000414   \n",
       "1             1878.224854        17613.091377  ...          0.000251   \n",
       "2            -1408.515747         3016.430184  ...          0.000431   \n",
       "3            -1293.191162         1691.972493  ...          0.000416   \n",
       "4            -2068.633545         1141.670493  ...          0.000233   \n",
       "...                   ...                 ...  ...               ...   \n",
       "7655                  inf                 NaN  ...          0.000201   \n",
       "7656                  inf                 NaN  ...          0.000279   \n",
       "7657                  inf                 NaN  ...          0.000268   \n",
       "7658             7.209878         2403.935707  ...          0.000246   \n",
       "7659                  inf                 NaN  ...          0.000198   \n",
       "\n",
       "      spread_diff1_max  spread_diff1_mean  spread_diff1_std  spread_diff1_sum  \\\n",
       "0             0.001551           0.001031          0.000223          0.311224   \n",
       "1             0.000954           0.000536          0.000192          0.107207   \n",
       "2             0.001388           0.000921          0.000246          0.173237   \n",
       "3             0.001989           0.001049          0.000381          0.125890   \n",
       "4             0.001026           0.000588          0.000156          0.103437   \n",
       "...                ...                ...               ...               ...   \n",
       "7655          0.001142           0.000755          0.000178          0.231676   \n",
       "7656          0.002508           0.001090          0.000365          0.538242   \n",
       "7657          0.001072           0.000789          0.000137          0.246919   \n",
       "7658          0.001353           0.000612          0.000224          0.265700   \n",
       "7659          0.000990           0.000402          0.000127          0.122611   \n",
       "\n",
       "      spread_diff2_min  spread_diff2_max  spread_diff2_mean  spread_diff2_std  \\\n",
       "0             0.000517          0.001500           0.001006          0.000185   \n",
       "1             0.000251          0.001054           0.000529          0.000170   \n",
       "2             0.000479          0.001628           0.000923          0.000252   \n",
       "3             0.000555          0.001666           0.000968          0.000262   \n",
       "4             0.000233          0.000979           0.000506          0.000150   \n",
       "...                ...               ...                ...               ...   \n",
       "7655          0.000336          0.001142           0.000716          0.000161   \n",
       "7656          0.000209          0.002159           0.001056          0.000335   \n",
       "7657          0.000447          0.001250           0.000780          0.000158   \n",
       "7658          0.000246          0.001476           0.000654          0.000242   \n",
       "7659          0.000198          0.000891           0.000402          0.000125   \n",
       "\n",
       "      spread_diff2_sum  \n",
       "0             0.303775  \n",
       "1             0.105850  \n",
       "2             0.173524  \n",
       "3             0.116127  \n",
       "4             0.089079  \n",
       "...                ...  \n",
       "7655          0.219720  \n",
       "7656          0.521524  \n",
       "7657          0.244062  \n",
       "7658          0.283659  \n",
       "7659          0.122609  \n",
       "\n",
       "[7660 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_book_feature_set(train_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d84e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9060c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67a810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124a501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6912414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6b9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c39fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
