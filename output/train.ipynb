{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b7e675d",
   "metadata": {},
   "source": [
    "# Optiver Realized Volatility Prediction - Train\n",
    "\n",
    "**This notebook seeks to EDITS HERE**\n",
    "---------\n",
    "\n",
    "## Files\n",
    "**book_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Provides order book data on the most competitive buy and sell orders entered into the market. The top two levels of the book are shared. The first level of the book will be more competitive in price terms, it will then receive execution priority over the second level.\n",
    "\n",
    " - `stock_id` - ID code for the stock. Not all `stock_id`s exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n",
    " - `time_id` - ID code for the time bucket. `time_id`s are not necessarily sequential but are consistent across all stocks.\n",
    " - `seconds_in_bucket` - Number of seconds from the start of the bucket, always starting from 0.\n",
    " - `bid_price[1/2]` - Normalized prices of the most/second most competitive buy level.\n",
    " - `ask_price[1/2]` - Normalized prices of the most/second most competitive sell level.\n",
    " - `bid_size[1/2]` - The number of shares on the most/second most competitive buy level.\n",
    " - `ask_size[1/2]` - The number of shares on the most/second most competitive sell level.\n",
    " \n",
    "**trade_[train/test].parquet** - A [parquet](https://arrow.apache.org/docs/python/parquet.html) file partitioned by `stock_id`. Contains data on trades that actually executed. Usually, in the market, there are more passive buy/sell intention updates (book updates) than actual trades, therefore one may expect this file to be more sparse than the order book.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `seconds_in_bucket` - Same as above. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n",
    " - `price` - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n",
    " - `size` - The sum number of shares traded.\n",
    " - `order_count` - The number of unique trade orders taking place.\n",
    " \n",
    "**train.csv** The ground truth values for the training set.\n",
    "\n",
    " - `stock_id` - Same as above, but since this is a csv the column will load as an integer instead of categorical.\n",
    " - `time_id` - Same as above.\n",
    " - `target` - The realized volatility computed over the 10 minute window following the feature data under the same `stock_id`/`time_id`. There is no overlap between feature and target data. \n",
    " \n",
    "**test.csv** Provides the mapping between the other data files and the submission file. As with other test files, most of the data is only available to your notebook upon submission with just the first few rows available for download.\n",
    "\n",
    " - `stock_id` - Same as above.\n",
    " - `time_id` - Same as above.\n",
    " - `row_id` - Unique identifier for the submission row. There is one row for each existing `stock_id`/`time_id` pair. Each time window is not necessarily containing every individual stock.\n",
    " \n",
    "**sample_submission.csv** - A sample submission file in the correct format.\n",
    "\n",
    " - `row_id` - Same as in test.csv.\n",
    " - `target` - Same definition as in **train.csv**. The benchmark is using the median target value from **train.csv**.\n",
    " \n",
    "## Prepare Environment\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcab26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq # To handle parquet files\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data vis packages\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Data prep\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Modelling packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as k\n",
    "# Key layers\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Flatten\n",
    "# Activation layers\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU, ELU, ThresholdedReLU\n",
    "# Dropout layers\n",
    "from tensorflow.keras.layers import Dropout, AlphaDropout, GaussianDropout\n",
    "# Normalisation layers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Embedding layers\n",
    "from tensorflow.keras.layers import Embedding, Concatenate, Reshape\n",
    "# Callbacks\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "# Optimisers\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "# Model cross validation and evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "# For Bayesian hyperparameter searching\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de666dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "\n",
    "# Data access\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "\n",
    "# Get number of cpu cores for multiprocessing\n",
    "try:\n",
    "    cpus = int(multiprocessing.cpu_count() / 2)\n",
    "except NotImplementedError:\n",
    "    cpus = 1 # Default number of cores\n",
    "    \n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(f\"Num CPU Threads Available: {cpus}\")\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e027214",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ebc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "comp_dir_path = Path(\"../input/optiver-realized-volatility-prediction\")\n",
    "\n",
    "# Train paths\n",
    "train_book_path   = comp_dir_path/\"book_train.parquet\"\n",
    "train_trade_path  = comp_dir_path/\"trade_train.parquet\"\n",
    "train_labels_path = comp_dir_path/\"train.csv\"\n",
    "\n",
    "# Test paths\n",
    "test_book_path   = comp_dir_path/\"book_test.parquet\"\n",
    "test_trade_path  = comp_dir_path/\"trade_test.parquet\"\n",
    "test_labels_path = comp_dir_path/\"test.csv\"\n",
    "\n",
    "# Sample submission path\n",
    "sample_sub_path = comp_dir_path/\"sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d99a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for data reading\n",
    "def get_stock_ids_list(data_dir_path):\n",
    "    data_dir = os.listdir(data_dir_path)\n",
    "    # Get list of stock ids in directory\n",
    "    stock_ids = list(map(lambda x: x.split(\"=\")[1], data_dir))\n",
    "    return stock_ids\n",
    "    \n",
    "    \n",
    "def load_book_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        book_stock_id_path = os.path.join(train_book_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        book_stock_id_path = os.path.join(test_book_path, stock_id_ext)\n",
    "    book_stock_id = pd.read_parquet(book_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    book_stock_id[\"stock_id\"] = int(stock_id)\n",
    "            \n",
    "    return book_stock_id\n",
    "\n",
    "def load_trade_stock_id_data(stock_id):\n",
    "    # Get stock id extension\n",
    "    stock_id_ext = f\"stock_id={stock_id}\"\n",
    "    \n",
    "    # Read individual stock parquet file\n",
    "    if is_train_test == \"train\":\n",
    "        trade_stock_id_path = os.path.join(train_trade_path, stock_id_ext)\n",
    "    elif is_train_test == \"test\":\n",
    "        trade_stock_id_path = os.path.join(test_trade_path, stock_id_ext)\n",
    "    trade_stock_id = pd.read_parquet(trade_stock_id_path)\n",
    "    \n",
    "    # Add stock id feature from filename\n",
    "    trade_stock_id[\"stock_id\"] = int(stock_id)\n",
    "            \n",
    "    return trade_stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394aedc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get list of stock ids\n",
    "train_stock_ids = get_stock_ids_list(train_book_path)\n",
    "test_stock_ids = get_stock_ids_list(test_book_path)\n",
    "\n",
    "# Read train data\n",
    "is_train_test = \"train\"\n",
    "# Create worker pool and read\n",
    "pool         = multiprocessing.Pool(processes=cpus)\n",
    "train_book   = pd.concat(pool.map(load_book_stock_id_data, train_stock_ids[0:2]))\n",
    "train_trade  = pd.concat(pool.map(load_trade_stock_id_data, train_stock_ids[0:2]))\n",
    "train_labels = pd.read_csv(train_labels_path)\n",
    "# Close worker pool\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Read test data\n",
    "is_train_test = \"test\"\n",
    "# Create worker pool and read\n",
    "pool        = multiprocessing.Pool(processes=cpus)\n",
    "test_book   = pd.concat(pool.map(load_book_stock_id_data, test_stock_ids))\n",
    "test_trade  = pd.concat(pool.map(load_trade_stock_id_data, test_stock_ids))\n",
    "test_labels = pd.read_csv(test_labels_path)\n",
    "\n",
    "# Read sample submission\n",
    "sample_sub = pd.read_csv(sample_sub_path)\n",
    "\n",
    "# Print data dimensions\n",
    "print(\"TRAIN DATA DIMENSIONS\")\n",
    "print(f\"train_book shape: {train_book.shape}\")\n",
    "print(f\"train_trade shape: {train_trade.shape}\")\n",
    "print(f\"train_labels shape: {train_labels.shape}\")\n",
    "\n",
    "print(\"\\nTEST DATA DIMENSIONS\")\n",
    "print(f\"test_book shape: {test_book.shape}\")\n",
    "print(f\"test_trade shape: {test_trade.shape}\")\n",
    "print(f\"test_labels shape: {test_labels.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7b7fa",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### Define Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f51566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for data manipulation\n",
    "def get_log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff()\n",
    "\n",
    "\n",
    "def get_trade_log_return(df_trade, col_stock_id, col_time_id, col_price):\n",
    "    \"\"\"\n",
    "    Returns the Log Return at each time ID.\n",
    "    \"\"\"\n",
    "    trade_log_return = df_trade.groupby([col_stock_id, col_time_id])[col_price].apply(get_log_return)\n",
    "    trade_log_return = trade_log_return.fillna(0)\n",
    "    return trade_log_return\n",
    "\n",
    "\n",
    "def get_agg_feature(df, col_name, func):\n",
    "    \"\"\"\n",
    "    Returns aggregated feature by stock ID and time ID based on input df and feature.\n",
    "    \"\"\"\n",
    "    if \"function\" in str(func):\n",
    "        func_str = str(func).split(\" \")[1]\n",
    "        agg_feat_col_name = f\"{col_name}_{func_str}\"\n",
    "    else:\n",
    "        agg_feat_col_name = f\"{col_name}_{func}\"\n",
    "    \n",
    "    agg_feat = df.groupby(by=[\"stock_id\", \"time_id\"])[col_name].agg(func)\n",
    "    agg_feat = agg_feat.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    agg_feat = agg_feat.reset_index().rename(columns={col_name: agg_feat_col_name})\n",
    "    \n",
    "    return agg_feat\n",
    "\n",
    "\n",
    "def get_wap(df_book, col_bid_price, col_ask_price, col_bid_size, col_ask_size):\n",
    "    \"\"\"\n",
    "    Returns Weighted Average Price. \n",
    "    \"\"\"\n",
    "    wap_numerator = df_book[col_bid_price]  * df_book[col_ask_size]\n",
    "    wap_numerator += df_book[col_ask_price] * df_book[col_bid_size]\n",
    "    \n",
    "    wap_denominator = df_book[col_bid_size] + df_book[col_ask_size]\n",
    "    \n",
    "    return wap_numerator / wap_denominator\n",
    "\n",
    "\n",
    "def get_wap_combined(df_book, col_bid_price1, col_ask_price1, col_bid_size1, col_ask_size1,\n",
    "                     col_bid_price2, col_ask_price2, col_bid_size2, col_ask_size2):    \n",
    "    \"\"\"\n",
    "    Returns the Combined Weighted Average Price for both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    wap_numerator1  = df_book[col_bid_price1] * df_book[col_ask_size1]\n",
    "    wap_numerator1 += df_book[col_ask_price1] * df_book[col_bid_size1]\n",
    "    wap_numerator2  = df_book[col_bid_price2] * df_book[col_ask_size2]\n",
    "    wap_numerator2 += df_book[col_ask_price2] * df_book[col_bid_size2]\n",
    "    \n",
    "    wap_denominator  = df_book[col_bid_size1] + df_book[col_ask_size1]\n",
    "    wap_denominator += df_book[col_bid_size2] + df_book[col_ask_size2]\n",
    "    \n",
    "    return (wap_numerator1 + wap_numerator2) / wap_denominator\n",
    "\n",
    "\n",
    "def get_wap_avg(df_book, col_bid_price1, col_ask_price1, col_bid_size1, col_ask_size1,\n",
    "                col_bid_price2, col_ask_price2, col_bid_size2, col_ask_size2):\n",
    "    \"\"\"\n",
    "    Returns the Combined Average Weighted Average Price for both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    wap_numerator1  = df_book[col_bid_price1] * df_book[col_ask_size1]\n",
    "    wap_numerator1 += df_book[col_ask_price1] * df_book[col_bid_size1]\n",
    "    wap_numerator1 /= df_book[col_bid_size1] + df_book[col_ask_size1]\n",
    "    \n",
    "    wap_numerator2  = df_book[col_bid_price2] * df_book[col_ask_size2]\n",
    "    wap_numerator2 += df_book[col_ask_price2] * df_book[col_bid_size2]\n",
    "    wap_numerator2 /= df_book[col_bid_size2] + df_book[col_ask_size2]\n",
    "    \n",
    "    return (wap_numerator1 + wap_numerator2) / 2\n",
    "\n",
    "\n",
    "def get_vol_wap(df_book, col_stock_id, col_time_id, col_wap):\n",
    "    \"\"\"\n",
    "    Returns the Volume Weighted Average Price at each time ID.\n",
    "    \"\"\"\n",
    "    vol_wap = df_book.groupby([col_stock_id, col_time_id])[col_wap].apply(get_log_return)\n",
    "    vol_wap = vol_wap.fillna(0)\n",
    "    return vol_wap\n",
    "\n",
    "\n",
    "def get_bid_ask_spread(df_book, col_bid_price1, col_ask_price1, col_bid_price2, col_ask_price2):\n",
    "    \"\"\"\n",
    "    Get Combined bid ask spread using both Bid and Ask features.\n",
    "    \"\"\"\n",
    "    bas_numerator   = df_book[[col_ask_price1, col_ask_price2]].min(axis=1)\n",
    "    bas_denominator = df_book[[col_bid_price1, col_bid_price2]].max(axis=1) - 1\n",
    "    \n",
    "    return bas_numerator / bas_denominator\n",
    "\n",
    "\n",
    "def get_vertical_spread(df_book, col_price1, col_price2):\n",
    "    \"\"\"\n",
    "    Returns the vertical spread for Bid/Ask price features inputted.\n",
    "    \"\"\"\n",
    "    v_spread = df_book[col_price1] - df_book[col_price2]\n",
    "    return v_spread\n",
    "\n",
    "\n",
    "def get_spread_feature(df_book, col_price_a, col_price_b):\n",
    "    \"\"\"\n",
    "    Returns a spread feature based on the price features inputted.\n",
    "    \"\"\"\n",
    "    spread_feat = df_book[col_price_a] - df_book[col_price_b]\n",
    "    return spread_feat\n",
    "\n",
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    \"\"\"\n",
    "    Returns the realized volatility for a given period.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(series_log_return**2))\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Returns the Root Mean Squared Prediction Error.\n",
    "    \"\"\"\n",
    "    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "    return rmspe\n",
    "\n",
    "\n",
    "def get_row_id(df, col_stock_id, col_time_id):\n",
    "    \"\"\"\n",
    "    Returns row ids in format required for submission. \n",
    "    \"\"\"\n",
    "    row_ids = df[col_stock_id].astype(\"str\") + \"-\" + df[col_time_id].astype(\"str\")\n",
    "    return row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile data manipulation helper functions into complete functions\n",
    "def extract_trade_feature_set(df_trade):\n",
    "    \"\"\"\n",
    "    Returns engineered trade dataset, where each row is a unique stock ID/time ID pair.\n",
    "    \"\"\"\n",
    "    # Get the Log return for trades by stock ID and time ID\n",
    "    df_trade[\"trade_log_return\"] = get_trade_log_return(df_trade, \"stock_id\", \"time_id\", \"price\")\n",
    "    \n",
    "    # Get aggregate statistics for specified numerical features\n",
    "    trade_features = [\"price\", \"size\", \"order_count\", \"trade_log_return\"]\n",
    "    \n",
    "    for trade_feature in trade_features:\n",
    "        # Get min aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"min\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get max aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"max\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get mean aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"mean\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get std aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"std\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get sum aggregations\n",
    "        df_trade = df_trade.merge(\n",
    "            get_agg_feature(df=df_trade, col_name=trade_feature, func=\"sum\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "    \n",
    "    # Reduce trade df to just unique stock ID and time ID pairs\n",
    "    df_trade = df_trade.drop([\"seconds_in_bucket\", \"price\", \"size\", \"order_count\", \"trade_log_return\"], axis=1)\n",
    "    df_trade = df_trade.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_trade\n",
    "\n",
    "\n",
    "def extract_book_feature_set(df_book):\n",
    "    \"\"\"\n",
    "    Returns engineered book dataset, where each row is a unique stock ID/time ID pair.\n",
    "    \"\"\"\n",
    "    # WAP for both bid/ask price/size features\n",
    "    df_book[\"wap1\"] = get_wap(df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\")\n",
    "    df_book[\"wap2\"] = get_wap(df_book, \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\")\n",
    "    # Combined WAP\n",
    "    df_book[\"wap_combined\"] = get_wap_combined(\n",
    "        df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\", \n",
    "        \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\"\n",
    "    )\n",
    "    # Average WAP for both bid/ask price/size features\n",
    "    df_book[\"wap_avg\"] = get_wap_avg(\n",
    "        df_book, \"bid_price1\", \"ask_price1\", \"bid_size1\", \"ask_size1\", \n",
    "        \"bid_price2\", \"ask_price2\", \"bid_size2\", \"ask_size2\"\n",
    "    )\n",
    "    \n",
    "    # Get VWAPS based on different WAP features\n",
    "    df_book[\"vol_wap1\"]         = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap1\")\n",
    "    df_book[\"vol_wap2\"]         = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap2\")\n",
    "    df_book[\"vol_wap_combined\"] = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap_combined\")\n",
    "    df_book[\"vol_wap_avg\"]      = get_vol_wap(df_book, \"stock_id\", \"time_id\", \"wap_avg\")\n",
    "    \n",
    "    # Get different spread features\n",
    "    df_book[\"bid_ask_spread\"] = get_bid_ask_spread(df_book, \"bid_price1\", \"ask_price1\", \"bid_price2\",\"ask_price2\")\n",
    "    df_book[\"bid_v_spread\"]   = get_vertical_spread(df_book, \"bid_price1\", \"bid_price2\")\n",
    "    df_book[\"ask_v_spread\"]   = get_vertical_spread(df_book, \"ask_price1\", \"ask_price2\")\n",
    "    df_book[\"h_spread1\"]      = get_spread_feature(df_book, \"ask_price1\", \"bid_price1\")\n",
    "    df_book[\"h_spread2\"]      = get_spread_feature(df_book, \"ask_price2\", \"bid_price2\")\n",
    "    df_book[\"spread_diff1\"]   = get_spread_feature(df_book, \"ask_price1\", \"bid_price2\")\n",
    "    df_book[\"spread_diff2\"]   = get_spread_feature(df_book, \"ask_price2\", \"bid_price1\")\n",
    "    \n",
    "    # Get aggregated volatility features for each VWAP\n",
    "    vol_features = [\"vol_wap1\", \"vol_wap2\", \"vol_wap_combined\", \"vol_wap_avg\"]\n",
    "    \n",
    "    for vol_feature in vol_features:\n",
    "         df_book = df_book.merge(\n",
    "             get_agg_feature(df=df_book, col_name=vol_feature, func=realized_volatility),\n",
    "             how=\"left\",\n",
    "             on=[\"stock_id\", \"time_id\"]\n",
    "         )\n",
    "            \n",
    "    # Get aggregated features for different spread features\n",
    "    spread_features = [\n",
    "        \"bid_ask_spread\", \"bid_v_spread\", \"ask_v_spread\", \"h_spread1\", \n",
    "        \"h_spread2\", \"spread_diff1\", \"spread_diff2\"\n",
    "    ]\n",
    "    \n",
    "    for spread_feature in spread_features:\n",
    "        # Get min aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"min\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get max aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"max\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get mean aggregations\n",
    "        df_book = df_book.merge(\n",
    "             get_agg_feature(df=df_book, col_name=spread_feature, func=\"mean\"),\n",
    "             how=\"left\",\n",
    "             on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get std aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"std\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "        # Get sum aggregations\n",
    "        df_book = df_book.merge(\n",
    "            get_agg_feature(df=df_book, col_name=spread_feature, func=\"sum\"),\n",
    "            how=\"left\",\n",
    "            on=[\"stock_id\", \"time_id\"]\n",
    "        )\n",
    "\n",
    "    # Reduce trade df to just unique stock ID and time ID pairs\n",
    "    df_book = df_book.drop([\n",
    "        \"seconds_in_bucket\", \"bid_price1\", \"ask_price1\", \"bid_price2\", \n",
    "        \"ask_price2\", \"bid_size1\", \"ask_size1\", \"bid_size2\", \"ask_size2\",\n",
    "        # WAP features\n",
    "        \"wap1\", \"wap2\", \"wap_combined\", \"wap_avg\", \"vol_wap1\", \n",
    "        \"vol_wap2\", \"vol_wap_combined\", \"vol_wap_avg\", \n",
    "        # Spread features\n",
    "        \"bid_ask_spread\", \"bid_v_spread\", \"ask_v_spread\", \"h_spread1\", \n",
    "        \"h_spread2\", \"spread_diff1\", \"spread_diff2\" \n",
    "    ], axis=1)\n",
    "    df_book = df_book.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_book\n",
    "\n",
    "\n",
    "def get_initial_feature_set(df_train, df_trade, df_book):\n",
    "    \"\"\"\n",
    "    Returns engineered feature set with labels, before preprocessing\n",
    "    \"\"\"\n",
    "    # Extract trade and book features\n",
    "    df_trade = extract_trade_feature_set(df_trade)\n",
    "    df_book  = extract_book_feature_set(df_book)\n",
    "    # Merge trade and book features to labels\n",
    "    df_train = pd.merge(df_train, df_trade, how=\"inner\", on=[\"stock_id\", \"time_id\"])\n",
    "    df_train = pd.merge(df_train, df_book, how=\"inner\", on=[\"stock_id\", \"time_id\"])\n",
    "    \n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271b102",
   "metadata": {},
   "source": [
    "### Full Data Manipulation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run feature generation pipeline\n",
    "train = get_initial_feature_set(train_labels, train_trade, train_book)\n",
    "\n",
    "X_tdx = train[0:999].drop(\"target\", axis=1)\n",
    "X_vdx = train[1000:1999].drop(\"target\", axis=1)\n",
    "y_tdx = train[0:999][\"target\"]\n",
    "y_vdx = train[1000:1999][\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key parameters\n",
    "baseline_model = True\n",
    "\n",
    "SEED = 14\n",
    "np.random.seed(SEED)\n",
    "\n",
    "SCALER_METHOD = RobustScaler()\n",
    "\n",
    "FEATURE_SELECTOR = RandomForestRegressor(random_state=SEED)\n",
    "NUM_FEATURES = 500\n",
    "\n",
    "PCA_METHOD = PCA(random_state=SEED)\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "KFOLDS = 2\n",
    "PATIENCE = 10\n",
    "\n",
    "MODEL_TO_USE = \"nn\"\n",
    "model_name_save = f\"{MODEL_TO_USE}_final_classifier_seed_{str(SEED)}\"\n",
    "\n",
    "print(f\"Model name: {model_name_save}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define full dataset transformation pipeline\n",
    "def transform_dataset(X_train, X_val, y_train, y_val, \n",
    "                      verbose=0, \n",
    "                      scaler=SCALER_METHOD, \n",
    "                      feature_selector=FEATURE_SELECTOR,\n",
    "                      num_features=NUM_FEATURES,\n",
    "                      pca=PCA_METHOD, \n",
    "                      seed=SEED\n",
    "                     ):\n",
    "    \"\"\"\n",
    "    Takes in train and validation datasets, and applies feature transformations,\n",
    "    feature selection, scaling and pca (dependent on arguments). \n",
    "    \n",
    "    Returns transformed X_train and X_val data ready for training/prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    ## DATA PREPARATION ##\n",
    "\n",
    "    # Get indices for train and validation dfs - we'll need these later\n",
    "    train_idx = list(X_train.index)\n",
    "    val_idx   = list(X_val.index)\n",
    "       \n",
    "    # Get train colnames before scaling and feature selection (minus ID features)\n",
    "    feat_cols = X_train.drop([\"stock_id\", \"time_id\"], axis=1).columns\n",
    "    \n",
    "    # Get subset for ID features\n",
    "    train_id_feats = X_train[[\"stock_id\", \"time_id\"]]\n",
    "    val_id_feats = X_val[[\"stock_id\", \"time_id\"]]\n",
    "    \n",
    "    \n",
    "    ## SCALING ##\n",
    "    \n",
    "    if scaler != None:\n",
    "        if verbose == 1:\n",
    "            print(\"APPLYING SCALER...\")\n",
    "            \n",
    "        # Fit and transform scaler to train and val\n",
    "        scaler.fit(X_train.drop([\"stock_id\", \"time_id\"], axis=1))\n",
    "        X_train = scaler.transform(X_train.drop([\"stock_id\", \"time_id\"], axis=1))\n",
    "        X_val   = scaler.transform(X_val.drop([\"stock_id\", \"time_id\"], axis=1))\n",
    "        \n",
    "        # Convert scaled array back dataframe\n",
    "        X_train = pd.DataFrame(X_train, index=train_idx, columns=feat_cols)\n",
    "        X_train = pd.merge(train_id_feats, X_train, how=\"left\", left_index=True, right_index=True)\n",
    "        \n",
    "        X_val = pd.DataFrame(X_val, index=val_idx, columns=feat_cols)\n",
    "        X_val = pd.merge(val_id_feats, X_val, how=\"left\", left_index=True, right_index=True)\n",
    "\n",
    "        \n",
    "    ## FEATURE SELECTION ##\n",
    "    \n",
    "    # Feature selection is only ran on numerical data\n",
    "    if feature_selector != None:\n",
    "        if verbose == 1:\n",
    "            print(\"APPLYING FEATURE SELECTOR...\")\n",
    "            cols_num = X_train.shape[1]\n",
    "                \n",
    "        # Fit tree based classifier to select features\n",
    "        feature_selector_fit = SelectFromModel(estimator=feature_selector)\n",
    "        feature_selector_fit = feature_selector_fit.fit(X_train, y_train)\n",
    "        \n",
    "        # Retrieve the names of the features selected for each label\n",
    "        feature_idx = feature_selector_fit.get_support()\n",
    "        selected_features = list(X_train.columns[feature_idx])\n",
    "        \n",
    "        # Subset datasets to selected features only\n",
    "        X_train = X_train[selected_features]\n",
    "        X_val   = X_val[selected_features]\n",
    "        \n",
    "        if verbose == 1: \n",
    "            print(f\"{cols_num - X_train.shape[1]} features removed in feature selection.\")\n",
    "                \n",
    "        \n",
    "    ## PCA ##\n",
    "    \n",
    "    if pca != None:\n",
    "        if verbose == 1:\n",
    "            print(\"APPLYING PCA...\")\n",
    "        # Fit and transform pca to train and val\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_val   = pca.transform(X_val)\n",
    "        if verbose == 1:\n",
    "            print(f\"NUMBER OF PRINCIPAL COMPONENTS: {pca.n_components_}\")\n",
    "        # Convert numerical features into pandas dataframe and clean colnames\n",
    "        X_train = pd.DataFrame(X_train, index=train_idx).add_prefix(\"pca_\")\n",
    "        X_val   = pd.DataFrame(X_val, index=val_idx).add_prefix(\"pca_\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if verbose == 1:\n",
    "        print(f\"TRAIN SHAPE: \\t\\t{X_train.shape}\")\n",
    "        print(f\"VALIDATION SHAPE: \\t{X_val.shape}\")\n",
    "\n",
    "        \n",
    "    return X_train, X_val, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49152f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, selected_features = transform_dataset(X_tdx, X_vdx, y_tdx, y_vdx, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8be0784",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "### Learning Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start          = 0.00001, \n",
    "               lr_max            = 0.0008, \n",
    "               lr_min            = 0.00001, \n",
    "               lr_rampup_epochs  = 20, \n",
    "               lr_sustain_epochs = 0, \n",
    "               lr_exp_decay      = 0.8):\n",
    "    \n",
    "    lr_max = lr_max * strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "\n",
    "    return lrfn\n",
    "\n",
    "lrfn = build_lrfn()\n",
    "lr = LearningRateScheduler(lrfn, verbose=0)\n",
    "\n",
    "plt.plot([lrfn(epoch) for epoch in range(EPOCHS)])\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8d0bff",
   "metadata": {},
   "source": [
    "### Define Baseline Model\n",
    "The below model was the original architecture, however when we conduct our Bayesian Hyperparameter search, we'll be playing around with the architecture of this baseline model a little. Parameter tuning will affect the model depth as well as the numbers of nodes at each layer, the dropout layers, activation functions and optimisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df964dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baseline_model == True:\n",
    "    def get_model(X_train, y_train):\n",
    "        \n",
    "        input_ = Input(shape=(X_train.shape[1], ))\n",
    "        x = Dense(8192, activation='relu')(input_)\n",
    "            \n",
    "        x = Dense(4096, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x) \n",
    "        \n",
    "        x = Dense(2048, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "    \n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        x = Dense(512, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "        model = Model(input_, output)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df684e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c77343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a726f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05077697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16743a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2792d62d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7d2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473e602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fea284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c775d0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c616b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dabdb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568e62ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab3e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fb9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6912414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6b9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c39fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
